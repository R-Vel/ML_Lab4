{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96205ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\d3lus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\d3lus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# Download stopwords if not present\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Setup Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "print(f\"--> Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5bfe039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Loading data...\n",
      "--> Balancing Data...\n",
      "    Balanced Dataset Size: 18434\n",
      "--> Creating Cleaned Text for LDA...\n",
      "    Data preparation complete.\n"
     ]
    }
   ],
   "source": [
    "# 1. Load Data\n",
    "print(\"--> Loading data...\")\n",
    "try:\n",
    "    df_pd = pd.read_parquet('aita_train.parquet')\n",
    "except FileNotFoundError:\n",
    "    # Dummy data for testing if file missing\n",
    "    print(\"    Warning: File not found. Generating Dummy Data.\")\n",
    "    df_pd = pd.DataFrame({\n",
    "        'title': ['My neighbor is loud'] * 100 + ['I donated to charity'] * 100,\n",
    "        'text': ['He plays drums all night.'] * 100 + ['It felt good to help.'] * 100,\n",
    "        'verdict': ['yta'] * 100 + ['nta'] * 100\n",
    "    })\n",
    "\n",
    "# 2. Basic Preprocessing\n",
    "df_pd['title'] = df_pd['title'].fillna('')\n",
    "df_pd['text'] = df_pd['text'].fillna('')\n",
    "df_pd['full_text_raw'] = df_pd['title'] + \" \" + df_pd['text']\n",
    "\n",
    "# Map Labels\n",
    "label_map = {'nta': 0, 'nah': 0, 'yta': 1, 'esh': 1}\n",
    "df_pd['label'] = df_pd['verdict'].map(label_map)\n",
    "df_pd = df_pd.dropna(subset=['label'])\n",
    "df_pd['label'] = df_pd['label'].astype(int)\n",
    "\n",
    "# 3. PROFESSOR FEEDBACK: UNDERSAMPLING\n",
    "# Balance the dataset to ~18k rows total (9k each) to make training feasible\n",
    "print(\"--> Balancing Data...\")\n",
    "df_yta = df_pd[df_pd['label'] == 1]\n",
    "df_nta = df_pd[df_pd['label'] == 0]\n",
    "\n",
    "# Undersample majority class\n",
    "if len(df_nta) > len(df_yta):\n",
    "    df_nta = df_nta.sample(n=len(df_yta), random_state=42)\n",
    "else:\n",
    "    df_yta = df_yta.sample(n=len(df_nta), random_state=42)\n",
    "\n",
    "df_balanced = pd.concat([df_yta, df_nta]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(f\"    Balanced Dataset Size: {len(df_balanced)}\")\n",
    "\n",
    "# 4. PROFESSOR FEEDBACK: STOP WORD REMOVAL (For LDA Only)\n",
    "print(\"--> Creating Cleaned Text for LDA...\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text_for_lda(text):\n",
    "    # Lowercase, remove non-alpha, remove stopwords\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    words = text.split()\n",
    "    words = [w for w in words if w not in stop_words and len(w) > 2]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Apply cleaning (This might take a few seconds)\n",
    "df_balanced['lda_text'] = df_balanced['full_text_raw'].apply(clean_text_for_lda)\n",
    "print(\"    Data preparation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ca4fce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Running LDA Topic Modeling...\n",
      "    Training LDA Model 1 on NTA data (7834 docs)...\n",
      "    Training LDA Model 2 on YTA data (7834 docs)...\n",
      "--> Generating Topic Features for entire dataset...\n",
      "    LDA Features Shape: (18434, 20)\n"
     ]
    }
   ],
   "source": [
    "# --- PROFESSOR FEEDBACK: LDA TOPIC MODELING ---\n",
    "print(\"--> Running LDA Topic Modeling...\")\n",
    "\n",
    "# Parameters\n",
    "N_TOPICS_PER_CLASS = 10 # You will get 10 features from YTA model + 10 from NTA model = 20 total extra features\n",
    "\n",
    "# Split data for LDA training (We only train LDA on the training set portion to avoid leakage!)\n",
    "train_df_temp, _ = train_test_split(df_balanced, test_size=0.15, stratify=df_balanced['label'], random_state=42)\n",
    "\n",
    "# Separate documents by class\n",
    "docs_nta = train_df_temp[train_df_temp['label'] == 0]['lda_text']\n",
    "docs_yta = train_df_temp[train_df_temp['label'] == 1]['lda_text']\n",
    "\n",
    "print(f\"    Training LDA Model 1 on NTA data ({len(docs_nta)} docs)...\")\n",
    "# Vectorize\n",
    "vectorizer_nta = CountVectorizer(max_features=2000, max_df=0.9, min_df=5)\n",
    "X_nta = vectorizer_nta.fit_transform(docs_nta)\n",
    "# Train LDA\n",
    "lda_nta = LatentDirichletAllocation(n_components=N_TOPICS_PER_CLASS, random_state=42, n_jobs=-1)\n",
    "lda_nta.fit(X_nta)\n",
    "\n",
    "print(f\"    Training LDA Model 2 on YTA data ({len(docs_yta)} docs)...\")\n",
    "# Vectorize\n",
    "vectorizer_yta = CountVectorizer(max_features=2000, max_df=0.9, min_df=5)\n",
    "X_yta = vectorizer_yta.fit_transform(docs_yta)\n",
    "# Train LDA\n",
    "lda_yta = LatentDirichletAllocation(n_components=N_TOPICS_PER_CLASS, random_state=42, n_jobs=-1)\n",
    "lda_yta.fit(X_yta)\n",
    "\n",
    "print(\"--> Generating Topic Features for entire dataset...\")\n",
    "\n",
    "# Helper function to get topic distribution\n",
    "def get_lda_features_dual(text_series):\n",
    "    # Stream 1: How much does this look like NTA topics?\n",
    "    vec_nta = vectorizer_nta.transform(text_series)\n",
    "    topics_nta = lda_nta.transform(vec_nta) # Shape (n_samples, 10)\n",
    "    \n",
    "    # Stream 2: How much does this look like YTA topics?\n",
    "    vec_yta = vectorizer_yta.transform(text_series)\n",
    "    topics_yta = lda_yta.transform(vec_yta) # Shape (n_samples, 10)\n",
    "    \n",
    "    # Combine: Shape (n_samples, 20)\n",
    "    return np.hstack([topics_nta, topics_yta])\n",
    "\n",
    "# Calculate features for the whole dataframe\n",
    "lda_features_all = get_lda_features_dual(df_balanced['lda_text'])\n",
    "\n",
    "# Add to dataframe (store as list or numpy array column is tricky, so we keep it separate index-aligned)\n",
    "print(f\"    LDA Features Shape: {lda_features_all.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6833319a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Sample of Data with LDA Features:\n",
      "  verdict                                           lda_text  NTA_Topic_1  \\\n",
      "0     yta  aita dont say sounds fun sarcastically basical...     0.000532   \n",
      "1     nta  wibta hid future income parents live fathermd ...     0.000417   \n",
      "2     esh  aita handled work conflict year old trans man ...     0.105370   \n",
      "3     nta  aita called stubborn wanting help chores boyfr...     0.106076   \n",
      "4     yta  aita telling return cats recently adopted two ...     0.000538   \n",
      "\n",
      "   NTA_Topic_2  YTA_Topic_1  YTA_Topic_2  \n",
      "0     0.000532     0.000535     0.000535  \n",
      "1     0.000417     0.000419     0.045794  \n",
      "2     0.000495     0.131215     0.221067  \n",
      "3     0.000585     0.105307     0.000599  \n",
      "4     0.000538     0.000538     0.107960  \n",
      "\n",
      "--> Detailed view of the first row:\n",
      "NTA_Topic_1     0.000532\n",
      "NTA_Topic_2     0.000532\n",
      "NTA_Topic_3     0.771054\n",
      "NTA_Topic_4     0.000532\n",
      "NTA_Topic_5     0.000532\n",
      "NTA_Topic_6     0.000532\n",
      "NTA_Topic_7     0.000532\n",
      "NTA_Topic_8     0.000532\n",
      "NTA_Topic_9     0.000532\n",
      "NTA_Topic_10     0.22469\n",
      "YTA_Topic_1     0.000535\n",
      "YTA_Topic_2     0.000535\n",
      "YTA_Topic_3     0.000535\n",
      "YTA_Topic_4     0.000535\n",
      "YTA_Topic_5     0.000535\n",
      "YTA_Topic_6     0.000535\n",
      "YTA_Topic_7     0.000535\n",
      "YTA_Topic_8     0.571611\n",
      "YTA_Topic_9      0.42411\n",
      "YTA_Topic_10    0.000535\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Create a temporary 'view' to inspect the LDA features alongside the text\n",
    "df_inspection = df_balanced.copy()\n",
    "\n",
    "# 1. Create column names for the 20 topics (10 from NTA model, 10 from YTA model)\n",
    "topic_cols = [f\"NTA_Topic_{i+1}\" for i in range(10)] + [f\"YTA_Topic_{i+1}\" for i in range(10)]\n",
    "\n",
    "# 2. Add the LDA probabilities as columns\n",
    "df_lda_probs = pd.DataFrame(lda_features_all, columns=topic_cols)\n",
    "df_inspection = pd.concat([df_inspection, df_lda_probs], axis=1)\n",
    "\n",
    "# 3. View the text alongside its topic probabilities\n",
    "print(\"--> Sample of Data with LDA Features:\")\n",
    "# We select just the text, the verdict, and the first few topic columns to display\n",
    "cols_to_show = ['verdict', 'lda_text', 'NTA_Topic_1', 'NTA_Topic_2', 'YTA_Topic_1', 'YTA_Topic_2']\n",
    "print(df_inspection[cols_to_show].head(5))\n",
    "\n",
    "# Optional: Check if a specific row has high probability for a specific topic\n",
    "print(\"\\n--> Detailed view of the first row:\")\n",
    "print(df_inspection.iloc[0][topic_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc72e1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Top words for NTA Topic Models:\n",
      "   NTA Topic #1: husband, wife, baby, home, time, aita, told, food, house, said\n",
      "   NTA Topic #2: family, wedding, birthday, party, want, would, dont, christmas, day, aita\n",
      "   NTA Topic #3: friend, said, friends, didnt, told, aita, got, would, asked, like\n",
      "   NTA Topic #4: money, pay, would, job, work, get, rent, help, aita, move\n",
      "   NTA Topic #5: work, time, like, dont, get, aita, feel, know, one, would\n",
      "   NTA Topic #6: told, said, mom, family, like, dad, mother, aita, didnt, dont\n",
      "   NTA Topic #7: kids, sister, parents, dont, want, family, daughter, children, school, would\n",
      "   NTA Topic #8: mom, dad, brother, car, got, get, told, parents, house, back\n",
      "   NTA Topic #9: room, dog, house, get, home, aita, back, one, door, dont\n",
      "   NTA Topic #10: like, said, dont, aita, something, really, one, get, didnt, say\n",
      "--------------------------------------------------\n",
      "--> Top words for YTA Topic Models:\n",
      "   YTA Topic #1: mom, sister, told, wife, dad, said, didnt, parents, got, family\n",
      "   YTA Topic #2: work, school, job, time, would, get, one, class, dont, like\n",
      "   YTA Topic #3: would, room, move, apartment, house, roommate, moved, place, one, live\n",
      "   YTA Topic #4: gift, like, said, get, got, really, didnt, one, dont, bought\n",
      "   YTA Topic #5: dog, get, room, time, like, house, dont, home, work, day\n",
      "   YTA Topic #6: brother, husband, kids, son, daughter, family, said, dont, told, like\n",
      "   YTA Topic #7: money, pay, car, get, would, back, parents, dont, house, help\n",
      "   YTA Topic #8: said, didnt, told, friend, got, asked, back, friends, time, would\n",
      "   YTA Topic #9: like, friend, friends, said, dont, told, really, know, didnt, one\n",
      "   YTA Topic #10: wedding, family, want, would, food, dont, eat, dinner, also, time\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def print_top_words(model, vectorizer, n_top_words=10, prefix=\"Topic\"):\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    print(f\"--> Top words for {prefix} Models:\")\n",
    "    \n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        # Sort the words by importance (highest weight first)\n",
    "        top_indices = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_words = [feature_names[i] for i in top_indices]\n",
    "        \n",
    "        print(f\"   {prefix} #{topic_idx+1}: {', '.join(top_words)}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Print NTA Topics\n",
    "print_top_words(lda_nta, vectorizer_nta, prefix=\"NTA Topic\")\n",
    "\n",
    "# Print YTA Topics\n",
    "print_top_words(lda_yta, vectorizer_yta, prefix=\"YTA Topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5eb5fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Model Architecture defined.\n"
     ]
    }
   ],
   "source": [
    "# 1. Prepare Split\n",
    "train_idx, test_idx = train_test_split(np.arange(len(df_balanced)), test_size=0.15, stratify=df_balanced['label'], random_state=42)\n",
    "train_idx, val_idx = train_test_split(train_idx, test_size=0.15, stratify=df_balanced.iloc[train_idx]['label'], random_state=42)\n",
    "\n",
    "# 2. Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# 3. Custom Dataset Class\n",
    "class HybridDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, lda_features, indices, tokenizer, max_len=512):\n",
    "        self.df = df.iloc[indices].reset_index(drop=True)\n",
    "        self.lda_data = lda_features[indices] # Select corresponding LDA rows\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        row = self.df.iloc[item]\n",
    "        text = row['full_text_raw']\n",
    "        label = row['label']\n",
    "        lda_vec = self.lda_data[item]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'lda_features': torch.tensor(lda_vec, dtype=torch.float), # The new input\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create Datasets\n",
    "train_ds = HybridDataset(df_balanced, lda_features_all, train_idx, tokenizer)\n",
    "val_ds = HybridDataset(df_balanced, lda_features_all, val_idx, tokenizer)\n",
    "\n",
    "# Create Loaders (Keep Batch Size low for GPU safety)\n",
    "BATCH_SIZE = 8\n",
    "ACCUMULATION_STEPS = 2 \n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "# 4. PROFESSOR FEEDBACK: ARCHITECTURE EXPERIMENTATION\n",
    "# Custom Hybrid Model\n",
    "class BertWithLDA(nn.Module):\n",
    "    def __init__(self, n_lda_features):\n",
    "        super(BertWithLDA, self).__init__()\n",
    "        # Load base BERT model (outputs 768 dim vector)\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        \n",
    "        # Final Classifier: BERT (768) + LDA (20) -> 2 Classes\n",
    "        combined_dim = 768 + n_lda_features\n",
    "        self.out = nn.Linear(combined_dim, 2)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, lda_features):\n",
    "        # 1. Feed text to BERT\n",
    "        bert_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        # pooler_output is the embedding of the [CLS] token (representation of whole sentence)\n",
    "        pooled_output = bert_output.pooler_output \n",
    "        output = self.drop(pooled_output)\n",
    "        \n",
    "        # 2. Concatenate BERT output with LDA probabilities\n",
    "        # output shape: [batch, 768], lda_features shape: [batch, 20]\n",
    "        combined_output = torch.cat((output, lda_features), dim=1)\n",
    "        \n",
    "        # 3. Classify\n",
    "        return self.out(combined_output)\n",
    "\n",
    "print(\"--> Model Architecture defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8c8471c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\d3lus\\AppData\\Local\\Temp\\ipykernel_37024\\3899505410.py:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler() # Mixed Precision\n",
      "C:\\Users\\d3lus\\AppData\\Local\\Temp\\ipykernel_37024\\3899505410.py:25: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Starting Hybrid Training (3 Epochs)...\n",
      "Epoch 1/3\n",
      "----------\n",
      "    Batch 0/1665 loss: 0.6588\n",
      "    Batch 200/1665 loss: 0.6919\n",
      "    Batch 400/1665 loss: 0.5720\n",
      "    Batch 600/1665 loss: 0.7745\n",
      "    Batch 800/1665 loss: 0.7252\n",
      "    Batch 1000/1665 loss: 0.5834\n",
      "    Batch 1200/1665 loss: 0.7769\n",
      "    Batch 1400/1665 loss: 0.5355\n",
      "    Batch 1600/1665 loss: 0.4741\n",
      "  Train loss 0.6696 accuracy 0.5886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\d3lus\\AppData\\Local\\Temp\\ipykernel_37024\\3899505410.py:65: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Val   loss 0.6317 accuracy 0.6482\n",
      "\n",
      "Epoch 2/3\n",
      "----------\n",
      "    Batch 0/1665 loss: 0.5981\n",
      "    Batch 200/1665 loss: 0.7601\n",
      "    Batch 400/1665 loss: 0.6822\n",
      "    Batch 600/1665 loss: 0.3750\n",
      "    Batch 800/1665 loss: 0.6796\n",
      "    Batch 1000/1665 loss: 0.5126\n",
      "    Batch 1200/1665 loss: 0.5642\n",
      "    Batch 1400/1665 loss: 0.7215\n",
      "    Batch 1600/1665 loss: 0.3960\n",
      "  Train loss 0.6002 accuracy 0.6777\n",
      "  Val   loss 0.6614 accuracy 0.6521\n",
      "\n",
      "Epoch 3/3\n",
      "----------\n",
      "    Batch 0/1665 loss: 0.4534\n",
      "    Batch 200/1665 loss: 0.1600\n",
      "    Batch 400/1665 loss: 0.4600\n",
      "    Batch 600/1665 loss: 0.4797\n",
      "    Batch 800/1665 loss: 0.3385\n",
      "    Batch 1000/1665 loss: 0.4392\n",
      "    Batch 1200/1665 loss: 1.0580\n",
      "    Batch 1400/1665 loss: 0.2474\n",
      "    Batch 1600/1665 loss: 0.2603\n",
      "  Train loss 0.4814 accuracy 0.7737\n",
      "  Val   loss 0.6898 accuracy 0.6601\n",
      "\n",
      "--> Training Complete! Time: 25.55 min\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "n_lda_cols = lda_features_all.shape[1] # Should be 20\n",
    "model = BertWithLDA(n_lda_features=n_lda_cols)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "EPOCHS = 3\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "scaler = GradScaler() # Mixed Precision\n",
    "\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for i, d in enumerate(data_loader):\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        lda_features = d[\"lda_features\"].to(device)\n",
    "        targets = d[\"label\"].to(device)\n",
    "\n",
    "        with autocast():\n",
    "            # Pass BOTH text and lda features\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                lda_features=lda_features\n",
    "            )\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss = loss / ACCUMULATION_STEPS\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        losses.append(loss.item() * ACCUMULATION_STEPS)\n",
    "        \n",
    "        # Calculate Accuracy\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "\n",
    "        if (i + 1) % ACCUMULATION_STEPS == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            \n",
    "        if i % 200 == 0:\n",
    "            print(f\"    Batch {i}/{len(data_loader)} loss: {loss.item() * ACCUMULATION_STEPS:.4f}\")\n",
    "\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n",
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            lda_features = d[\"lda_features\"].to(device)\n",
    "            targets = d[\"label\"].to(device)\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    lda_features=lda_features\n",
    "                )\n",
    "                loss = loss_fn(outputs, targets)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n",
    "# --- TRAINING ---\n",
    "print(f\"--> Starting Hybrid Training ({EPOCHS} Epochs)...\")\n",
    "history = {'train_acc': [], 'train_loss': [], 'val_acc': [], 'val_loss': []}\n",
    "total_start = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "    \n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model, train_loader, loss_fn, optimizer, device, scheduler, len(train_ds)\n",
    "    )\n",
    "    print(f'  Train loss {train_loss:.4f} accuracy {train_acc:.4f}')\n",
    "\n",
    "    val_acc, val_loss = eval_model(\n",
    "        model, val_loader, loss_fn, device, len(val_ds)\n",
    "    )\n",
    "    print(f'  Val   loss {val_loss:.4f} accuracy {val_acc:.4f}')\n",
    "    print()\n",
    "\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "print(f\"--> Training Complete! Time: {(time.time() - total_start)/60:.2f} min\")\n",
    "\n",
    "# Save\n",
    "output_dir = './saved_hybrid_bert'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "torch.save(model.state_dict(), os.path.join(output_dir, 'model_state.bin'))\n",
    "print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2831498f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Calculating Majority Class Baseline (PCC)...\n",
      "    Total Validation Samples: 2351\n",
      "    Class Distribution: {np.int64(0): 1176, np.int64(1): 1175}\n",
      "    Majority Class: 0 (Count: 1176)\n",
      "------------------------------\n",
      "    Baseline Accuracy to Beat: 0.5002 (50.02%)\n",
      "    Your Model Accuracy:       0.6601 (66.01%)\n",
      "------------------------------\n",
      "✅ SUCCESS: The model has beaten the majority class baseline.\n",
      "   Improvement over baseline: +15.99%\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def check_baseline_performance(dataloader, model_accuracy):\n",
    "    print(\"--> Calculating Majority Class Baseline (PCC)...\")\n",
    "    \n",
    "    # 1. Collect all actual labels from the validation set\n",
    "    all_labels = []\n",
    "    for batch in dataloader:\n",
    "        targets = batch['label'] # Assuming your dict key is 'label'\n",
    "        all_labels.extend(targets.cpu().numpy())\n",
    "    \n",
    "    # 2. Determine the majority class\n",
    "    counts = Counter(all_labels)\n",
    "    majority_class_label = max(counts, key=counts.get)\n",
    "    majority_count = counts[majority_class_label]\n",
    "    total_samples = len(all_labels)\n",
    "    \n",
    "    # 3. Calculate Baseline Accuracy (ZeroR)\n",
    "    # This is the accuracy if we just guessed the most common label every time\n",
    "    baseline_acc = majority_count / total_samples\n",
    "    \n",
    "    print(f\"    Total Validation Samples: {total_samples}\")\n",
    "    print(f\"    Class Distribution: {dict(counts)}\")\n",
    "    print(f\"    Majority Class: {majority_class_label} (Count: {majority_count})\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"    Baseline Accuracy to Beat: {baseline_acc:.4f} ({baseline_acc*100:.2f}%)\")\n",
    "    print(f\"    Your Model Accuracy:       {model_accuracy:.4f} ({model_accuracy*100:.2f}%)\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # 4. Final Verdict\n",
    "    if model_accuracy > baseline_acc:\n",
    "        print(\"✅ SUCCESS: The model has beaten the majority class baseline.\")\n",
    "        print(f\"   Improvement over baseline: +{(model_accuracy - baseline_acc)*100:.2f}%\")\n",
    "    else:\n",
    "        print(\"❌ WARNING: The model did not beat the baseline.\")\n",
    "        print(\"   Consider training longer or adjusting hyperparameters.\")\n",
    "\n",
    "# Run the check\n",
    "# 'val_loader' is your validation data loader\n",
    "# 'val_acc' is the final accuracy variable from your training loop\n",
    "check_baseline_performance(val_loader, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a136574e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
